import hashlib
import os
import re
import sys
import json
import datetime
from datetime import datetime
import akshare as ak
import requests
from bs4 import BeautifulSoup
from src.tools.openrouter_config import get_chat_completion, logger as api_logger
import time
import pandas as pd

def fetch_news_from_bing(symbol, start_date, end_date, max_news=10):
    query = f"{symbol} è‚¡ç¥¨ æ–°é—» site:finance.sina.com.cn"
    search_url = f"https://www.bing.com/search?q={query}"
    headers = {
        "User-Agent": "Mozilla/5.0"
    }

    print(f"ğŸ” æœç´¢å…³é”®è¯: {query}")
    response = requests.get(search_url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    result_blocks = soup.select('.b_algo')

    news_list = []
    for block in result_blocks:
        if len(news_list) >= max_news:
            break
        try:
            title = block.find('h2').get_text(strip=True)
            url = block.find('a')['href']
            print(f"â¡ï¸ æ­£åœ¨æŠ“å–æ­£æ–‡: {title} [{url}]")

            article_content, pub_time = extract_article_content(url)
            if not article_content or not pub_time:
                continue

            # å‘å¸ƒæ—¶é—´è¿‡æ»¤
            pub_dt = datetime.strptime(pub_time, "%Y-%m-%d")
            if pub_dt < datetime.strptime(start_date, "%Y-%m-%d") or pub_dt > datetime.strptime(end_date, "%Y-%m-%d"):
                continue

            news_item = {
                "title": title,
                "content": article_content,
                "publish_time": pub_time,
                "url": url,
                "source": "Bing+Sina"
            }
            news_list.append(news_item)

            time.sleep(1)  # é¿å…é¢‘ç¹è¯·æ±‚è¢«å°

        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥: {e}")
            continue

    return news_list

def extract_article_content(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')

        # æ–°é—»å†…å®¹æå–
        content_div = soup.find('div', class_='article') or soup.find('div', id='artibody')
        if not content_div:
            return None, None

        paragraphs = content_div.find_all('p')
        content = "\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))

        # å‘å¸ƒæ—¶é—´æå–
        text = soup.get_text()
        pub_time_match = re.search(r"(\d{4}-\d{2}-\d{2})\s+(\d{2}:\d{2})?", text)
        pub_date = pub_time_match.group(1) if pub_time_match else None

        return content, pub_date
    except Exception as e:
        print(f"âš ï¸ æå–å†…å®¹å¤±è´¥: {e}")
        return None, None
# def get_stock_news(symbol: str, max_news: int = 10) -> list:
#     """è·å–å¹¶å¤„ç†ä¸ªè‚¡æ–°é—»

#     Args:
#         symbol (str): è‚¡ç¥¨ä»£ç ï¼Œå¦‚ "300059"
#         max_news (int, optional): è·å–çš„æ–°é—»æ¡æ•°ï¼Œé»˜è®¤ä¸º10æ¡ã€‚æœ€å¤§æ”¯æŒ100æ¡ã€‚

#     Returns:
#         list: æ–°é—»åˆ—è¡¨ï¼Œæ¯æ¡æ–°é—»åŒ…å«æ ‡é¢˜ã€å†…å®¹ã€å‘å¸ƒæ—¶é—´ç­‰ä¿¡æ¯
#     """

#     # è®¾ç½®pandasæ˜¾ç¤ºé€‰é¡¹ï¼Œç¡®ä¿æ˜¾ç¤ºå®Œæ•´å†…å®¹
#     pd.set_option('display.max_columns', None)
#     pd.set_option('display.max_rows', None)
#     pd.set_option('display.max_colwidth', None)
#     pd.set_option('display.width', None)

#     # é™åˆ¶æœ€å¤§æ–°é—»æ¡æ•°
#     max_news = min(max_news, 100)

#     # è·å–å½“å‰æ—¥æœŸ
#     today = datetime.now().strftime("%Y-%m-%d")

#     # æ„å»ºæ–°é—»æ–‡ä»¶è·¯å¾„
#     # project_root = os.path.dirname(os.path.dirname(
#     #     os.path.dirname(os.path.abspath(__file__))))
#     news_dir = os.path.join("src", "data", "stock_news")
#     print(f"æ–°é—»ä¿å­˜ç›®å½•: {news_dir}")

#     # ç¡®ä¿ç›®å½•å­˜åœ¨
#     try:
#         os.makedirs(news_dir, exist_ok=True)
#         print(f"æˆåŠŸåˆ›å»ºæˆ–ç¡®è®¤ç›®å½•å­˜åœ¨: {news_dir}")
#     except Exception as e:
#         print(f"åˆ›å»ºç›®å½•å¤±è´¥: {e}")
#         return []

#     news_file = os.path.join(news_dir, f"{symbol}_news.json")
#     print(f"æ–°é—»æ–‡ä»¶è·¯å¾„: {news_file}")

#     # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æ–°é—»
#     need_update = True
#     if os.path.exists(news_file):
#         try:
#             with open(news_file, 'r', encoding='utf-8') as f:
#                 data = json.load(f)
#                 if data.get("date") == today:
#                     cached_news = data.get("news", [])
#                     if len(cached_news) >= max_news:
#                         print(f"ä½¿ç”¨ç¼“å­˜çš„æ–°é—»æ•°æ®: {news_file}")
#                         return cached_news[:max_news]
#                     else:
#                         print(
#                             f"ç¼“å­˜çš„æ–°é—»æ•°é‡({len(cached_news)})ä¸è¶³ï¼Œéœ€è¦è·å–æ›´å¤šæ–°é—»({max_news}æ¡)")
#         except Exception as e:
#             print(f"è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")

#     print(f'å¼€å§‹è·å–{symbol}çš„æ–°é—»æ•°æ®...')

#     try:
#         # è·å–æ–°é—»åˆ—è¡¨
#         news_df = ak.stock_news_em(symbol=symbol)
#         if news_df is None or len(news_df) == 0:
#             print(f"æœªè·å–åˆ°{symbol}çš„æ–°é—»æ•°æ®")
#             return []

#         print(f"æˆåŠŸè·å–åˆ°{len(news_df)}æ¡æ–°é—»")

#         # å®é™…å¯è·å–çš„æ–°é—»æ•°é‡
#         available_news_count = len(news_df)
#         if available_news_count < max_news:
#             print(f"è­¦å‘Šï¼šå®é™…å¯è·å–çš„æ–°é—»æ•°é‡({available_news_count})å°‘äºè¯·æ±‚çš„æ•°é‡({max_news})")
#             max_news = available_news_count

#         # è·å–æŒ‡å®šæ¡æ•°çš„æ–°é—»ï¼ˆè€ƒè™‘åˆ°å¯èƒ½æœ‰äº›æ–°é—»å†…å®¹ä¸ºç©ºï¼Œå¤šè·å–50%ï¼‰
#         news_list = []
#         for _, row in news_df.head(int(max_news * 1.5)).iterrows():
# #             try:
# #                 # è·å–æ–°é—»å†…å®¹
# #                 content = row["æ–°é—»å†…å®¹"] if "æ–°é—»å†…å®¹" in row and not pd.isna(
# #                     row["æ–°é—»å†…å®¹"]) else ""
# #                 if not content:
# #                     content = row["æ–°é—»æ ‡é¢˜"]

# #                 # åªå»é™¤é¦–å°¾ç©ºç™½å­—ç¬¦
# #                 content = content.strip()
# #                 if len(content) < 10:  # å†…å®¹å¤ªçŸ­çš„è·³è¿‡
# #                     continue

# #                 # è·å–å…³é”®è¯
# #                 keyword = row["å…³é”®è¯"] if "å…³é”®è¯" in row and not pd.isna(
# #                     row["å…³é”®è¯"]) else ""

# #                 # æ·»åŠ æ–°é—»
# #                 news_item = {
# #                     "title": row["æ–°é—»æ ‡é¢˜"].strip(),
# #                     "content": content,
# #                     "publish_time": row["å‘å¸ƒæ—¶é—´"],
# #                     "source": row["æ–‡ç« æ¥æº"].strip(),
# #                     "url": row["æ–°é—»é“¾æ¥"].strip(),
# #                     "keyword": keyword.strip()
# #                 }
# #                 news_list.append(news_item)
# #                 print(f"æˆåŠŸæ·»åŠ æ–°é—»: {news_item['title']}")

# #             except Exception as e:
# #                 print(f"å¤„ç†å•æ¡æ–°é—»æ—¶å‡ºé”™: {e}")
# #                 continue

# #         # æŒ‰å‘å¸ƒæ—¶é—´æ’åº
# #         #date_format = "%Y-%m-%d %H:%M:%S"
# #         #news_list=sorted(news_list, key=lambda x: datetime.strptime(x['publish_time'], date_format), reverse=True)

# #         #news_list.sort(key=lambda x: x["publish_time"], reverse=False)

# #         # åªä¿ç•™æŒ‡å®šæ¡æ•°çš„æœ‰æ•ˆæ–°é—»
# #         news_list = news_list[:max_news]

# #         # ä¿å­˜åˆ°æ–‡ä»¶
# #         try:
# #             save_data = {
# #                 "date": today,
# #                 "news": news_list
# #             }
# #             with open(news_file, 'w', encoding='utf-8') as f:
# #                 json.dump(save_data, f, ensure_ascii=False, indent=2)
# #             print(f"æˆåŠŸä¿å­˜{len(news_list)}æ¡çš„æ–°é—»åˆ°æ–‡ä»¶: {news_file}")
# #         except Exception as e:
# #             print(f"ä¿å­˜æ–°é—»æ•°æ®åˆ°æ–‡ä»¶æ—¶å‡ºé”™: {e}")

# #         return news_list

# #     except Exception as e:
# #         print(f"è·å–æ–°é—»æ•°æ®æ—¶å‡ºé”™: {e}")
# #         return []


# def get_news_sentiment(news_list: list, num_of_news: int = 5) -> float:
#     """åˆ†ææ–°é—»æƒ…æ„Ÿå¾—åˆ†

#     Args:
#         news_list (list): æ–°é—»åˆ—è¡¨
#         num_of_news (int): ç”¨äºåˆ†æçš„æ–°é—»æ•°é‡ï¼Œé»˜è®¤ä¸º5æ¡

#     Returns:
#         float: æƒ…æ„Ÿå¾—åˆ†ï¼ŒèŒƒå›´[-1, 1]ï¼Œ-1æœ€æ¶ˆæï¼Œ1æœ€ç§¯æ
#     """
#     if not news_list:
#         return 0.0

#     # # è·å–é¡¹ç›®æ ¹ç›®å½•
#     # project_root = os.path.dirname(os.path.dirname(
#     #     os.path.dirname(os.path.abspath(__file__))))

#     # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„æƒ…æ„Ÿåˆ†æç»“æœ
#     # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„æƒ…æ„Ÿåˆ†æç»“æœ
#     cache_file = "src/data/sentiment_cache.json"
#     os.makedirs(os.path.dirname(cache_file), exist_ok=True)
#     # news_key = hashlib.md5(news_list.encode('utf-8')).hexdigest()


#     news_key = "|".join([
#         f"{news['title']}|{news['content'][:100]}|{news['publish_time']}"
#         for news in news_list[:num_of_news]
#     ])

#     # æ£€æŸ¥ç¼“å­˜
#     if os.path.exists(cache_file):
#         print("å‘ç°æƒ…æ„Ÿåˆ†æç¼“å­˜æ–‡ä»¶")
#         try:
#             with open(cache_file, 'r', encoding='utf-8') as f:
#                 cache = json.load(f)
#                 if news_key in cache:
#                     print("ä½¿ç”¨ç¼“å­˜çš„æƒ…æ„Ÿåˆ†æç»“æœ")
#                     return cache[news_key]
#                 print("æœªæ‰¾åˆ°åŒ¹é…çš„æƒ…æ„Ÿåˆ†æç¼“å­˜")
#         except Exception as e:
#             print(f"è¯»å–æƒ…æ„Ÿåˆ†æç¼“å­˜å‡ºé”™: {e}")
#             cache = {}
#     else:
#         print("æœªæ‰¾åˆ°æƒ…æ„Ÿåˆ†æç¼“å­˜æ–‡ä»¶ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
#         cache = {}

#     # å‡†å¤‡ç³»ç»Ÿæ¶ˆæ¯
#     system_message = {
#         "role": "system",
#         "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Aè‚¡å¸‚åœºåˆ†æå¸ˆï¼Œæ“…é•¿è§£è¯»æ–°é—»å¯¹è‚¡ç¥¨èµ°åŠ¿çš„å½±å“ã€‚ä½ éœ€è¦åˆ†æä¸€ç»„æ–°é—»çš„æƒ…æ„Ÿå€¾å‘ï¼Œå¹¶ç»™å‡ºä¸€ä¸ªä»‹äº-1åˆ°1ä¹‹é—´çš„åˆ†æ•°ï¼š
#         - 1è¡¨ç¤ºæå…¶ç§¯æï¼ˆä¾‹å¦‚ï¼šé‡å¤§åˆ©å¥½æ¶ˆæ¯ã€è¶…é¢„æœŸä¸šç»©ã€è¡Œä¸šæ”¿ç­–æ”¯æŒï¼‰
#         - 0.5åˆ°0.9è¡¨ç¤ºç§¯æï¼ˆä¾‹å¦‚ï¼šä¸šç»©å¢é•¿ã€æ–°é¡¹ç›®è½åœ°ã€è·å¾—è®¢å•ï¼‰
#         - 0.1åˆ°0.4è¡¨ç¤ºè½»å¾®ç§¯æï¼ˆä¾‹å¦‚ï¼šå°é¢åˆåŒç­¾è®¢ã€æ—¥å¸¸ç»è¥æ­£å¸¸ï¼‰
#         - 0è¡¨ç¤ºä¸­æ€§ï¼ˆä¾‹å¦‚ï¼šæ—¥å¸¸å…¬å‘Šã€äººäº‹å˜åŠ¨ã€æ— é‡å¤§å½±å“çš„æ–°é—»ï¼‰
#         - -0.1åˆ°-0.4è¡¨ç¤ºè½»å¾®æ¶ˆæï¼ˆä¾‹å¦‚ï¼šå°é¢è¯‰è®¼ã€éæ ¸å¿ƒä¸šåŠ¡äºæŸï¼‰
#         - -0.5åˆ°-0.9è¡¨ç¤ºæ¶ˆæï¼ˆä¾‹å¦‚ï¼šä¸šç»©ä¸‹æ»‘ã€é‡è¦å®¢æˆ·æµå¤±ã€è¡Œä¸šæ”¿ç­–æ”¶ç´§ï¼‰
#         - -1è¡¨ç¤ºæå…¶æ¶ˆæï¼ˆä¾‹å¦‚ï¼šé‡å¤§è¿è§„ã€æ ¸å¿ƒä¸šåŠ¡ä¸¥é‡äºæŸã€è¢«ç›‘ç®¡å¤„ç½šï¼‰

#         åˆ†ææ—¶é‡ç‚¹å…³æ³¨ï¼š
#         1. ä¸šç»©ç›¸å…³ï¼šè´¢æŠ¥ã€ä¸šç»©é¢„å‘Šã€è¥æ”¶åˆ©æ¶¦ç­‰
#         2. æ”¿ç­–å½±å“ï¼šè¡Œä¸šæ”¿ç­–ã€ç›‘ç®¡æ”¿ç­–ã€åœ°æ–¹æ”¿ç­–ç­‰
#         3. å¸‚åœºè¡¨ç°ï¼šå¸‚åœºä»½é¢ã€ç«äº‰æ€åŠ¿ã€å•†ä¸šæ¨¡å¼ç­‰
#         4. èµ„æœ¬è¿ä½œï¼šå¹¶è´­é‡ç»„ã€è‚¡æƒæ¿€åŠ±ã€å®šå¢é…è‚¡ç­‰
#         5. é£é™©äº‹ä»¶ï¼šè¯‰è®¼ä»²è£ã€å¤„ç½šã€å€ºåŠ¡ç­‰
#         6. è¡Œä¸šåœ°ä½ï¼šæŠ€æœ¯åˆ›æ–°ã€ä¸“åˆ©ã€å¸‚å ç‡ç­‰
#         7. èˆ†è®ºç¯å¢ƒï¼šåª’ä½“è¯„ä»·ã€ç¤¾ä¼šå½±å“ç­‰

#         è¯·ç¡®ä¿åˆ†æï¼š
#         1. æ–°é—»çš„çœŸå®æ€§å’Œå¯é æ€§
#         2. æ–°é—»çš„æ—¶æ•ˆæ€§å’Œå½±å“èŒƒå›´
#         3. å¯¹å…¬å¸åŸºæœ¬é¢çš„å®é™…å½±å“
#         4. Aè‚¡å¸‚åœºçš„ç‰¹æ®Šååº”è§„å¾‹"""
#     }

#     # å‡†å¤‡æ–°é—»å†…å®¹
#     news_content = "\n\n".join([
#         f"æ ‡é¢˜ï¼š{news['title']}\n"
#         f"æ¥æºï¼š{news['source']}\n"
#         f"æ—¶é—´ï¼š{news['publish_time']}\n"
#         f"å†…å®¹ï¼š{news['content']}"
#         for news in news_list[:num_of_news]  # ä½¿ç”¨æŒ‡å®šæ•°é‡çš„æ–°é—»
#     ])

#     user_message = {
#         "role": "user",
#         "content": f"è¯·åˆ†æä»¥ä¸‹Aè‚¡ä¸Šå¸‚å…¬å¸ç›¸å…³æ–°é—»çš„æƒ…æ„Ÿå€¾å‘ï¼š\n\n{news_content}\n\nè¯·ç›´æ¥è¿”å›ä¸€ä¸ªæ•°å­—ï¼ŒèŒƒå›´æ˜¯-1åˆ°1ï¼Œæ— éœ€è§£é‡Šã€‚"
#     }

#     try:
#         # è·å–LLMåˆ†æç»“æœ
#         result = get_chat_completion([system_message, user_message])
#         if result is None:
#             print("Error: PI error occurred, LLM returned None")
#             return 0.0

#         # æå–æ•°å­—ç»“æœ
#         try:
#             sentiment_score = float(result.strip())
#         except ValueError as e:
#             print(f"Error parsing sentiment score: {e}")
#             print(f"Raw result: {result}")
#             return 0.0

#         # ç¡®ä¿åˆ†æ•°åœ¨-1åˆ°1ä¹‹é—´
#         sentiment_score = max(-1.0, min(1.0, sentiment_score))

#         # ç¼“å­˜ç»“æœ
#         cache[news_key] = sentiment_score
#         try:
#             with open(cache_file, 'w', encoding='utf-8') as f:
#                 json.dump(cache, f, ensure_ascii=False, indent=2)
#         except Exception as e:
#             print(f"Error writing cache: {e}")

#         return sentiment_score

#     except Exception as e:
#         print(f"Error analyzing news sentiment: {e}")
#         return 0.0  # å‡ºé”™æ—¶è¿”å›ä¸­æ€§åˆ†æ•°





def get_stock_news(symbol: str, start_date: str, end_date: str, max_news: int = 30) -> list:
    """è·å–å¹¶å¤„ç†ä¸ªè‚¡æ–°é—»ï¼ˆåŸºäºBingæœç´¢ + å®æ—¶æŠ“å–ï¼‰

    Args:
        symbol (str): è‚¡ç¥¨ä»£ç ï¼Œå¦‚ "300059"
        start_date (str): èµ·å§‹æ—¥æœŸï¼Œæ ¼å¼ä¸º "YYYY-MM-DD"
        end_date (str): ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ä¸º "YYYY-MM-DD"
        max_news (int, optional): è·å–çš„æ–°é—»æ¡æ•°ï¼Œé»˜è®¤ä¸º10æ¡ã€‚æœ€å¤§æ”¯æŒ100æ¡ã€‚

    Returns:
        list: æ–°é—»åˆ—è¡¨ï¼Œæ¯æ¡æ–°é—»åŒ…å«æ ‡é¢˜ã€å†…å®¹ã€å‘å¸ƒæ—¶é—´ç­‰ä¿¡æ¯
    """
    max_news = min(max_news, 100)
    news_list = []

    import requests
    from dateutil.parser import parse as date_parse

    endpoint = "https://api.bing.microsoft.com/v7.0/search"
    query = f"{symbol} è‚¡ç¥¨ æ–°é—»"
    headers = { 'Ocp-Apim-Subscription-Key': "9b355816f6264aa399db9d425a767a53" }
    params = {
        'q': query,
        'mkt': 'zh-CN',
        #'freshness': 'year',  # å¯é€‰ï¼šday, week, month
        'responseFilter': 'webpages',
        'count': max_news
    }

    try:
        response = requests.get(endpoint, headers=headers, params=params)
        response.raise_for_status()
        results = response.json()
        news_items = results.get("webPages", {}).get("value", [])
        news_list = []

        for item in news_items:
            try:
                pub_time = item.get("dateLastCrawled") or item.get("datePublished") or ""
                pub_dt = date_parse(pub_time).date() if pub_time else None
                if not pub_dt:
                    continue

                if pub_dt < datetime.strptime(start_date, "%Y-%m-%d").date() or pub_dt > datetime.strptime(end_date, "%Y-%m-%d").date():
                    continue

                news_list.append({
                    "title": item["name"],
                    "content": item["snippet"],
                    "publish_time": pub_dt.strftime("%Y-%m-%d"),
                    "source": item.get("provider", [{}])[0].get("name", "Bing"),
                    "url": item["url"],
                    "keyword": symbol
                })
            except Exception as inner_e:
                print(f"è§£ææ–°é—»å¤±è´¥: {inner_e}")
                continue
    
        return news_list
    except Exception as ex:
        print(f"Bing API è°ƒç”¨å¤±è´¥: {ex}")
        return []


def extract_article_content(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')

        content_div = soup.find('div', class_='article') or soup.find('div', id='artibody')
        if not content_div:
            return None, None

        paragraphs = content_div.find_all('p')
        content = "\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))

        text = soup.get_text()
        pub_time_match = re.search(r"(\d{4}-\d{2}-\d{2})", text)
        pub_date = pub_time_match.group(1) if pub_time_match else None

        return content, pub_date
    except Exception as e:
        print(f"âš ï¸ å†…å®¹æå–å¤±è´¥: {e}")
        return None, None


def get_news_sentiment(news_list: list, num_of_news: int = 5) -> float:
    if not news_list:
        return 0.0

    cache_file = "src/data/sentiment_cache.json"
    os.makedirs(os.path.dirname(cache_file), exist_ok=True)

    news_key = "|".join([
        f"{news['title']}|{news['content'][:100]}|{news['publish_time']}"
        for news in news_list[:num_of_news]
    ])

    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache = json.load(f)
                if news_key in cache:
                    return cache[news_key]
        except:
            cache = {}
    else:
        cache = {}

    system_message = {
        "role": "system",
        "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Aè‚¡å¸‚åœºåˆ†æå¸ˆï¼Œæ“…é•¿è§£è¯»æ–°é—»å¯¹è‚¡ç¥¨èµ°åŠ¿çš„å½±å“ã€‚ä½ éœ€è¦åˆ†æä¸€ç»„æ–°é—»çš„æƒ…æ„Ÿå€¾å‘ï¼Œå¹¶ç»™å‡ºä¸€ä¸ªä»‹äº-1åˆ°1ä¹‹é—´çš„åˆ†æ•°ï¼š
- 1è¡¨ç¤ºæå…¶ç§¯æï¼ˆä¾‹å¦‚ï¼šé‡å¤§åˆ©å¥½æ¶ˆæ¯ã€è¶…é¢„æœŸä¸šç»©ã€è¡Œä¸šæ”¿ç­–æ”¯æŒï¼‰
- 0.5åˆ°0.9è¡¨ç¤ºç§¯æï¼ˆä¾‹å¦‚ï¼šä¸šç»©å¢é•¿ã€æ–°é¡¹ç›®è½åœ°ã€è·å¾—è®¢å•ï¼‰
- 0.1åˆ°0.4è¡¨ç¤ºè½»å¾®ç§¯æï¼ˆä¾‹å¦‚ï¼šå°é¢åˆåŒç­¾è®¢ã€æ—¥å¸¸ç»è¥æ­£å¸¸ï¼‰
- 0è¡¨ç¤ºä¸­æ€§ï¼ˆä¾‹å¦‚ï¼šæ—¥å¸¸å…¬å‘Šã€äººäº‹å˜åŠ¨ã€æ— é‡å¤§å½±å“çš„æ–°é—»ï¼‰
- -0.1åˆ°-0.4è¡¨ç¤ºè½»å¾®æ¶ˆæï¼ˆä¾‹å¦‚ï¼šå°é¢è¯‰è®¼ã€éæ ¸å¿ƒä¸šåŠ¡äºæŸï¼‰
- -0.5åˆ°-0.9è¡¨ç¤ºæ¶ˆæï¼ˆä¾‹å¦‚ï¼šä¸šç»©ä¸‹æ»‘ã€é‡è¦å®¢æˆ·æµå¤±ã€è¡Œä¸šæ”¿ç­–æ”¶ç´§ï¼‰
- -1è¡¨ç¤ºæå…¶æ¶ˆæï¼ˆä¾‹å¦‚ï¼šé‡å¤§è¿è§„ã€æ ¸å¿ƒä¸šåŠ¡ä¸¥é‡äºæŸã€è¢«ç›‘ç®¡å¤„ç½šï¼‰

åˆ†ææ—¶é‡ç‚¹å…³æ³¨ï¼š
1. ä¸šç»©ç›¸å…³ï¼šè´¢æŠ¥ã€ä¸šç»©é¢„å‘Šã€è¥æ”¶åˆ©æ¶¦ç­‰
2. æ”¿ç­–å½±å“ï¼šè¡Œä¸šæ”¿ç­–ã€ç›‘ç®¡æ”¿ç­–ã€åœ°æ–¹æ”¿ç­–ç­‰
3. å¸‚åœºè¡¨ç°ï¼šå¸‚åœºä»½é¢ã€ç«äº‰æ€åŠ¿ã€å•†ä¸šæ¨¡å¼ç­‰
4. èµ„æœ¬è¿ä½œï¼šå¹¶è´­é‡ç»„ã€è‚¡æƒæ¿€åŠ±ã€å®šå¢é…è‚¡ç­‰
5. é£é™©äº‹ä»¶ï¼šè¯‰è®¼ä»²è£ã€å¤„ç½šã€å€ºåŠ¡ç­‰
6. è¡Œä¸šåœ°ä½ï¼šæŠ€æœ¯åˆ›æ–°ã€ä¸“åˆ©ã€å¸‚å ç‡ç­‰
7. èˆ†è®ºç¯å¢ƒï¼šåª’ä½“è¯„ä»·ã€ç¤¾ä¼šå½±å“ç­‰

è¯·ç¡®ä¿åˆ†æï¼š
1. æ–°é—»çš„çœŸå®æ€§å’Œå¯é æ€§
2. æ–°é—»çš„æ—¶æ•ˆæ€§å’Œå½±å“èŒƒå›´
3. å¯¹å…¬å¸åŸºæœ¬é¢çš„å®é™…å½±å“
4. Aè‚¡å¸‚åœºçš„ç‰¹æ®Šååº”è§„å¾‹"""
    }

    news_content = "\n\n".join([
        f"æ ‡é¢˜ï¼š{news['title']}\næ¥æºï¼š{news['source']}\næ—¶é—´ï¼š{news['publish_time']}\nå†…å®¹ï¼š{news['content']}"
        for news in news_list[:num_of_news]
    ])

    user_message = {
        "role": "user",
        "content": f"è¯·åˆ†æä»¥ä¸‹Aè‚¡ä¸Šå¸‚å…¬å¸ç›¸å…³æ–°é—»çš„æƒ…æ„Ÿå€¾å‘ï¼š\n\n{news_content}\n\nè¯·ç›´æ¥è¿”å›ä¸€ä¸ªæ•°å­—ï¼ŒèŒƒå›´æ˜¯-1åˆ°1ï¼Œæ— éœ€è§£é‡Šã€‚"
    }

    try:
        result = get_chat_completion([system_message, user_message])
        if result is None:
            return 0.0
        sentiment_score = float(result.strip())
        sentiment_score = max(-1.0, min(1.0, sentiment_score))
        cache[news_key] = sentiment_score
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
        return sentiment_score
    except:
        return 0.0


# if __name__ == "__main__":

#     news = get_stock_news('605108','2024-01-01', '2024-04-20')
#     print(news)
#     print(get_news_sentiment(news))
    # # print(get_stock_news("000001"))
    # news = get_stock_news("002987")
    # print(get_stock_news("002987"))
    # print(get_news_sentiment(news))
    # # print(get_news_sentiment(news_list))
